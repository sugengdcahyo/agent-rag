{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency imports\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to true this if you want to use cloudsql\n",
    "# USE_CLOUDSQL = False\n",
    "USE_CLOUDSQL = True\n",
    "\n",
    "project_id = \"imrenagi-gemini-experiment\" #change this to your project id\n",
    "region = \"us-central1\"\n",
    "gemini_embedding_model = \"text-embedding-004\"\n",
    "\n",
    "if not USE_CLOUDSQL:\n",
    "    # use pgvector docker image for local development\n",
    "    database_password = \"pyconapac\"\n",
    "    database_name = \"pyconapac\"\n",
    "    database_user = \"pyconapac\"\n",
    "    database_host = \"localhost\"\n",
    "else:\n",
    "    # use cloudsql credential if you want to use cloudsql\n",
    "    instance_name=\"pyconapac-demo\"\n",
    "    database_password = 'testing'\n",
    "    database_name = 'testing'\n",
    "    database_user = 'testing'\n",
    "\n",
    "assert database_name, \"⚠️ Please provide a database name\"\n",
    "assert database_user, \"⚠️ Please provide a database user\"\n",
    "assert database_password, \"⚠️ Please provide a database password\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ###Authenticate your Google Cloud Account and enable APIs.\n",
    "# Authenticate gcloud.\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {project_id}\n",
    "\n",
    "# Grant Cloud SQL Client role to authenticated user\n",
    "current_user = !gcloud auth list --filter=status:ACTIVE --format=\"value(account)\"\n",
    "print(f\"{current_user}\")\n",
    "# enable aiplatform apiservices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLOUDSQL:\n",
    "  print(f\"Granting Cloud SQL Client role to {current_user[0]}\")\n",
    "  # granting cloudsql client role to the current user\n",
    "  !gcloud projects add-iam-policy-binding {project_id} \\\n",
    "    --member=user:{current_user[0]} \\\n",
    "    --role=\"roles/cloudsql.client\"\n",
    "  # Enable Cloud SQL Admin API\n",
    "  !gcloud services enable sqladmin.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLOUDSQL:\n",
    "  #@markdown Create and setup a Cloud SQL PostgreSQL instance, if not done already.\n",
    "  database_version = !gcloud sql instances describe {instance_name} --format=\"value(databaseVersion)\"\n",
    "  if database_version[0].startswith(\"POSTGRES\"):\n",
    "    print(\"Found an existing Postgres Cloud SQL Instance!\")\n",
    "  else:\n",
    "    print(\"Creating new Cloud SQL instance...\")\n",
    "    !gcloud sql instances create {instance_name} --database-version=POSTGRES_15 \\\n",
    "      --region={region} --cpu=1 --memory=4GB --root-password={database_password} \\\n",
    "      --authorized-networks=0.0.0.0/0\n",
    "  # Create the database, if it does not exist.\n",
    "  out = !gcloud sql databases list --instance={instance_name} --filter=\"NAME:{database_name}\" --format=\"value(NAME)\"\n",
    "  if ''.join(out) == database_name:\n",
    "    print(\"Database %s already exists, skipping creation.\" % database_name)\n",
    "  else:\n",
    "    !gcloud sql databases create {database_name} --instance={instance_name}\n",
    "  # Create the database user for accessing the database.\n",
    "  !gcloud sql users create {database_user} \\\n",
    "    --instance={instance_name} \\\n",
    "    --password={database_password}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CLOUDSQL:\n",
    "    # get the ip address of the instance\n",
    "    ip_addresses = !gcloud sql instances describe {instance_name} --project {project_id} --format 'value(ipAddresses.ipAddress)'\n",
    "    # Split the IP addresses and take the first one\n",
    "    database_host = ip_addresses[0].split(';')[0].strip()\n",
    "    print(f\"Using database host: {database_host}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn_string = f\"postgres://{database_user}:{database_password}@{database_host}:5432/{database_name}\"\n",
    "db_conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file into a pandas DataFrame\n",
    "df = pd.read_json('course_content.jsonl', lines=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncpg\n",
    "\n",
    "async def main():\n",
    "    # Create connection to PostgreSQL database\n",
    "    conn = await asyncpg.connect(\n",
    "        host=database_host,\n",
    "        user=database_user,\n",
    "        password=database_password,\n",
    "        database=database_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        await conn.execute(\"DROP TABLE IF EXISTS course_contents CASCADE\")\n",
    "        # Create the `course_contents` table.\n",
    "        await conn.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS course_contents (\n",
    "                                id SERIAL PRIMARY KEY,\n",
    "                                title TEXT,\n",
    "                                content TEXT,\n",
    "                                file_path TEXT,\n",
    "                                slug TEXT\n",
    "                                )\"\"\"\n",
    "        )\n",
    "\n",
    "        # Create an index on the slug column for faster lookups\n",
    "        await conn.execute(\n",
    "            \"\"\"CREATE INDEX IF NOT EXISTS idx_course_contents_slug \n",
    "               ON course_contents (slug)\"\"\"\n",
    "        )\n",
    "\n",
    "        # Copy the dataframe to the `course_contents` table.\n",
    "        tuples = list(df.itertuples(index=False))\n",
    "        await conn.copy_records_to_table(\n",
    "            \"course_contents\", records=tuples, columns=list(df), timeout=10\n",
    "        )\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "# Run the SQL commands now.\n",
    "await main()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "text_splitter = MarkdownTextSplitter(\n",
    "  chunk_size=1000, \n",
    "  chunk_overlap=200)\n",
    "\n",
    "chunked = []\n",
    "for index, row in df.iterrows():\n",
    "    course_content_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    content = row[\"content\"]\n",
    "    splits = text_splitter.create_documents([content])\n",
    "    for s in splits:\n",
    "        r = {\"course_content_id\": course_content_id, \"content\": s.page_content}\n",
    "        chunked.append(r)\n",
    "\n",
    "chunked_df = pd.DataFrame(chunked)\n",
    "chunked_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import time\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=project_id, location=region)\n",
    "# Create a Vertex AI Embeddings service\n",
    "embeddings_service = VertexAIEmbeddings(model_name=gemini_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to retry failed API requests with exponential backoff.\n",
    "def retry_with_backoff(func, *args, retry_delay=5, backoff_factor=2, **kwargs):\n",
    "    max_attempts = 10\n",
    "    retries = 0\n",
    "    for i in range(max_attempts):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "            retries += 1\n",
    "            wait = retry_delay * (backoff_factor**retries)\n",
    "            print(f\"Retry after waiting for {wait} seconds...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "for i in range(0, len(chunked), batch_size):\n",
    "    request = [x[\"content\"] for x in chunked[i : i + batch_size]]\n",
    "    response = retry_with_backoff(embeddings_service.embed_documents, request)\n",
    "    # Store the retrieved vector embeddings for each chunk back.\n",
    "    for x, e in zip(chunked[i : i + batch_size], response):\n",
    "        x[\"embedding\"] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the generated embeddings in a pandas dataframe.\n",
    "course_content_embeddings = pd.DataFrame(chunked)\n",
    "course_content_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the generated vector embeddings in a PostgreSQL table.\n",
    "# This code may run for a few minutes.\n",
    "import numpy as np\n",
    "import asyncpg\n",
    "from pgvector.asyncpg import register_vector\n",
    "\n",
    "async def main():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=database_host,\n",
    "        user=database_user,\n",
    "        password=database_password,\n",
    "        database=database_name\n",
    "    )\n",
    "\n",
    "    # this is not used since we already use pgvector docker container\n",
    "    await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "    await register_vector(conn)\n",
    "\n",
    "    await conn.execute(\"DROP TABLE IF EXISTS course_content_embeddings\")\n",
    "    # Create the `product_embeddings` table to store vector embeddings.\n",
    "    await conn.execute(\n",
    "        \"\"\"CREATE TABLE IF NOT EXISTS course_content_embeddings(\n",
    "                            id INTEGER NOT NULL REFERENCES course_contents(id),\n",
    "                            content TEXT,\n",
    "                            embedding vector(768))\"\"\"\n",
    "    )\n",
    "\n",
    "    # Store all the generated embeddings back into the database.\n",
    "    for index, row in course_content_embeddings.iterrows():\n",
    "        await conn.execute(\n",
    "            \"INSERT INTO course_content_embeddings (id, content, embedding) VALUES ($1, $2, $3)\",\n",
    "            row[\"course_content_id\"],\n",
    "            row[\"content\"],\n",
    "            np.array(row[\"embedding\"]),\n",
    "        )\n",
    "\n",
    "    await conn.close()\n",
    "\n",
    "# Run the SQL commands now.\n",
    "await main()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create indexes for faster similarity search in pgvector\n",
    "\n",
    "- Vector indexes can significantly speed up similarity search operation and avoid the brute-force exact nearest neighbor search that is used by default.\n",
    "\n",
    "- pgvector comes with two types of indexes (as of v0.5.1): `hnsw` and `ivfflat`.\n",
    "\n",
    "> 💡 Click [here](https://cloud.google.com/blog/products/databases/faster-similarity-search-performance-with-pgvector-indexes) to learn more about pgvector indexes.\n",
    "\n",
    "Enter or modify the values of index parameters for your index of choice and run the corresponding cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Create an HNSW index on the `course_content_embeddings` table:\n",
    "m =  24 # @param {type:\"integer\"}\n",
    "ef_construction = 100  # @param {type:\"integer\"}\n",
    "operator =  \"vector_cosine_ops\"  # @param [\"vector_cosine_ops\", \"vector_l2_ops\", \"vector_ip_ops\"]\n",
    "\n",
    "# Quick input validations.\n",
    "assert m, \"⚠️ Please input a valid value for m.\"\n",
    "assert ef_construction, \"⚠️ Please input a valid value for ef_construction.\"\n",
    "assert operator, \"⚠️ Please input a valid value for operator.\"\n",
    "\n",
    "import asyncpg\n",
    "from pgvector.asyncpg import register_vector\n",
    "\n",
    "async def main():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=database_host,\n",
    "        user=database_user,\n",
    "        password=database_password,\n",
    "        database=database_name\n",
    "    )\n",
    "    await register_vector(conn)\n",
    "\n",
    "    # Create an HNSW index on the `course_content_embeddings` table.\n",
    "    await conn.execute(\n",
    "        f\"\"\"CREATE INDEX ON course_content_embeddings\n",
    "          USING hnsw(embedding {operator})\n",
    "          WITH (m = {m}, ef_construction = {ef_construction})\n",
    "        \"\"\"\n",
    "    )\n",
    "    await conn.close()\n",
    "\n",
    "\n",
    "# Run the SQL commands now.\n",
    "await main()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the best way to design forgot password\"  # @param {type:\"string\"}\n",
    "\n",
    "assert query, \"⚠️ Please input a valid input search text\"\n",
    "\n",
    "qe = embeddings_service.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the query embedding to a numpy array to inspect the content\n",
    "np.array(qe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.asyncpg import register_vector\n",
    "import asyncpg\n",
    "\n",
    "matches = []\n",
    "\n",
    "async def main():\n",
    "    conn = await asyncpg.connect(\n",
    "        host=database_host,\n",
    "        user=database_user,\n",
    "        password=database_password,\n",
    "        database=database_name\n",
    "    )\n",
    "    await register_vector(conn)\n",
    "    \n",
    "    similarity_threshold = 0.1\n",
    "    num_matches = 50\n",
    "\n",
    "    results = await conn.fetch(\n",
    "        \"\"\"\n",
    "                        WITH vector_matches AS (\n",
    "                          SELECT id, content, 1 - (embedding <=> $1) AS similarity\n",
    "                          FROM course_content_embeddings\n",
    "                          WHERE 1 - (embedding <=> $1) > $2\n",
    "                          ORDER BY similarity DESC\n",
    "                          LIMIT $3\n",
    "                        )\n",
    "                        SELECT cc.id as id, cc.title as title, \n",
    "                            vm.content as content, \n",
    "                            vm.similarity as similarity \n",
    "                        FROM course_contents cc\n",
    "                        LEFT JOIN vector_matches vm ON cc.id = vm.id;\n",
    "                        \"\"\",\n",
    "        qe,\n",
    "        similarity_threshold,\n",
    "        num_matches,\n",
    "        \n",
    "    )\n",
    "\n",
    "    if len(results) == 0:\n",
    "        raise Exception(\"Did not find any results. Adjust the query parameters.\")\n",
    "\n",
    "    for r in results:\n",
    "        # Collect the description for all the matched similar contents.\n",
    "        matches.append(\n",
    "            {\n",
    "                \"id\": r[\"id\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"content\": r[\"content\"],\n",
    "                \"similarity\": r[\"similarity\"],                \n",
    "            }\n",
    "        )\n",
    "\n",
    "    await conn.close()\n",
    "\n",
    "\n",
    "# Run the SQL commands now.\n",
    "await main()  # type: ignore\n",
    "\n",
    "matches = pd.DataFrame(matches)\n",
    "matches.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if USE_CLOUDSQL:\n",
    "#     !gcloud sql instances delete {instance_name} --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
